# Introduction to Data Lake Project 
This is a data engineering project to help the analytics team of a music company to build a data storage system. With this data properly stored, it will allow their analytics team to continue finding insights in what songs their users are listening to

# Project Description
The goal of this project to assist the need of the team is to build an ETL pipeline. To do this, I will:
* load the raw JSON file from S3 (`the data location`)
* Process the data using spark into analytics table
* load the data back to S3

# Dataset for the Projects
There are two datasets for the project, the song data, and the log data.
## Song Data
Song dataset contains the metadata about the song and the artist. It is in JSON format 

## Log data
This data is a JSON file generated by the event simulator based on the song dataset. These simulate activity logs from a music streaming app.



# Data Processing
Using the song and log datasets, the data will be processed into 5 different tables, each table holding different information that can give the team insights and can easily be used.

## Tables
**songplays —** records in log data associated with song plays i.e. records with page `NextSong`
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

**users —** users in the app
*user_id, first_name, last_name, gender, level*

**songs —** songs in the music database
*song_id, title, artist_id, year, duration*

**artists —** artists in the music database
*artist_id, name, location, latitude, longitude*

**time —** timestamps of records in songplays broken down into the specific unit
*start_time, hour, day, week, month, year, weekday*


# Project Template
The project includes:
1. `etl_test.ipynb` is a test jupyter notebook to interactively test run each data processing step before jumping into the ETL script writing. I do all the troubleshooting on the notebook
1. `etl.py` reads data from S3, processes that data into different tables using Spark, and writes them back to S3.
1. `dl.cfg` contains your AWS credentials
1. `README.md`  provides an overview of project code and steps

# Processing Data
## STEP 1 - Import necessary functions
To start the project, import necessary libraries to process the data and load in your AWS credentials in the config file

``` python
config = configparser.ConfigParser()
config.read(r"C:\DiT\CS\Data Engineer\Module 3 - Data lake\dl.cfg")

os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']
```

## STEP 2 - Create spark session
Since spark will be used, let create a module that will create spark session

```python
def create_spark_session():
    """Create a apache spark session."""
    spark = SparkSession.builder \
                .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.2") \
                    .appName("Using Spark on S3") \
                .getOrCreate()
    print("SparkSession Created!")
    return spark
```

## STEP 3 - process different data
To start data processing, I studied the data to know how to best approach the data processing pattern. After a brief overview of the data, I created modules to process the log data and the song data. The process is kinda straight forward.

```python
def process_song_data(spark, input_data, output_data):
    """
    This help process the song data into the different dimension table

    Parameters
    ----------
    spark: session
        This is the spark session that has been created
    input_data: path
        This is the path to the song_data s3 bucket.
    output_data: path
        This is the path that holds all saved files
    """
        .
        .
                        # Check ETL.py for complete code...
```


```python
def process_log_data(spark, input_data, output_data):
    """
    This help process the log data into the different dimension table

    Parameters
    ----------
    spark: session
        This is the spark session that has been created
    input_data: path
        This is the path to the log_data s3 bucket.
    output_data: path
        This is the path that holds all saved files
    """
        .
        .
                        # Check ETL.py for complete code...
```


`For time table`: I created a python function to process the data time data. Though I can easily use Pyspark in-built function

`For songplay table`: this is the complicated part of the project, the data needed for this table requires the two tables (`the log_data and the song_data`). To do this, I created a tempView for the table which made me work with tables as I would in SQL. I left joined the tables (log_df on left and song_data right) using the artist name as the reference

``` python
songplays_table = spark.sql("""
                    SELECT DISTINCT 
                                l.ts as start_time, 
                                l.userId as user_id, 
                                l.level, 
                                s.song_id, 
                                s.artist_id, 
                                l.sessionId as session_id,
                                l.location,
                                l.userAgent as user_agent,
                                l.year,
                                l.month                               
                    FROM 
                                log_df l
                    LEFT JOIN
                                song_df s ON l.artist = s.artist_name
                    """)
```


## STEP 4 - create the main program
Calls all modules, connect to spark, fetch data from s3
```python
def main():
    """
    Perform the following roles:
    1.) Get or create a spark session.
    1.) Read the song and log data from s3.
    2.) take the data and transform them into tables
    which will then be written to parquet files.
    3.) Load the parquet files on s3.
    """
```