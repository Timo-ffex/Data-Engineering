{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Import Necessary Library`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import findspark\n",
    "SPARK_HOME = os.getenv(\"SPARK_HOME\")\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"log-data\"\n",
    "log_df = spark.read.format('json').load(filepath)\n",
    "# df = spark.read.json(\"log-data/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.createOrReplaceTempView(\"log_df\")\n",
    "ts_col =  spark.sql(\"\"\"\n",
    "                    SELECT ts FROM log_df\n",
    "                    WHERE lower(page) = \"nextsong\"\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "@udf(MapType(StringType(), StringType()))\n",
    "def convert_ts(ts):\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "            The 'ts' parameter is a millisecond value from the log_data\n",
    "\n",
    "    Function:\n",
    "            Convert the timestamp in millisecond to the required format i.e\n",
    "            hour, day, week of year, month, year and weekday.\n",
    "    \"\"\"\n",
    "    \n",
    "    t = datetime.fromtimestamp(ts/1000)\n",
    "    try:\n",
    "        hour = t.hour\n",
    "        day =  t.day\n",
    "        week_of_year = t.isocalendar()[1]\n",
    "        month = t.month\n",
    "        year = t.year\n",
    "        weekday =  t.weekday()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return {\"start_time\"   : ts, \n",
    "                'hour'              : hour, \n",
    "                'day'                : day, \n",
    "                'week'              : week_of_year, \n",
    "                'month'            : month, \n",
    "                'year'               : year, \n",
    "                'weekday'         : weekday}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_col = ts_col.withColumn(\"parsed_ts\", convert_ts(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_col.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 200)\n",
    "ts_col.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']\n",
    "exprs = [f\"parsed_ts['{field}'] as {field}\" for field in fields ]\n",
    "exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the time table by mapping with headings\n",
    "time_t = ts_col.selectExpr(*exprs).dropDuplicates()\n",
    "time_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated values\n",
    "time_t = time_t.dropDuplicates()\n",
    "time_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_t.toPandas().to_csv(\"time.csv\") # save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Extracing User Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id, first_name, last_name, gender, level\n",
    "user_t = log_df.select([\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\"]).dropDuplicates()\n",
    "user_t.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_t.createOrReplaceTempView(\"user_t\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#     SELECT userId, count(*) as time_shown from user_t\n",
    "#     GROUP BY userId\n",
    "#     HAVING time_shown >1\n",
    "# \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT  * from user_t\n",
    "   WHERE userId = 15 or userId = 29 or userId = 85 or userId = \"\"\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading song-data into dataFrame\n",
    "Here, I created Schema for the data so as to get the datatype correctly. This is a step you will want to consider very often if you are loading data into relational database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField , DoubleType, IntegerType ,  StringType, TimestampType\n",
    "songSchema =  StructType([\n",
    "                        StructField(\"artist_id\", StringType()),\n",
    "                        StructField(\"artist_latitude\", DoubleType()),\n",
    "                        StructField(\"artist_location\", StringType()),\n",
    "                        StructField(\"artist_longitude\", DoubleType()),\n",
    "                        StructField(\"artist_name\", StringType()),\n",
    "                        StructField(\"duration\", DoubleType()),\n",
    "                        StructField(\"num_songs\", IntegerType()),\n",
    "                        StructField(\"song_id\", StringType()),\n",
    "                        StructField(\"title\", StringType()),\n",
    "                        StructField(\"year\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Dataframe\n",
    "This step is not needed, I used a simplier method in the compiled code but this is just another way to load data from different directory and how to concatenate dataframe in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\DiT\\CS\\Data Engineer\\Module 3 - Data lake\\Data lake project\\song-data\"\n",
    "\n",
    "#  This gets all the json files in the mother directory\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    files = glob.glob(os.path.join(root,'*.json'))\n",
    "    for f in files :\n",
    "        all_files.append(os.path.abspath(f))\n",
    "\n",
    "# this loads all the json file as a dataframe and append the dataframe into a list\n",
    "dfs = []\n",
    "for what in all_files:\n",
    "    a = spark.read.json(what, schema=songSchema)\n",
    "    # print(a.count())\n",
    "    dfs.append(a)\n",
    "\n",
    "# this joins all the dataframe in the list together\n",
    "song_df =  reduce(lambda first, second: first.union(second), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Extracting Song Table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_id, title, artist_id, year, duration\n",
    "song_t = song_df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"])\n",
    "song_t.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# song_df.describe()\n",
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Extracting Artist Table for the song-data`\n",
    "``` spark\n",
    "root\n",
    " |-- artist_id: string (nullable = true)\n",
    " |-- artist_latitude: double (nullable = true)\n",
    " |-- artist_location: string (nullable = true)\n",
    " |-- artist_longitude: double (nullable = true)\n",
    " |-- artist_name: string (nullable = true)\n",
    " |-- duration: double (nullable = true)\n",
    " |-- num_songs: integer (nullable = true)\n",
    " |-- song_id: string (nullable = true)\n",
    " |-- title: string (nullable = true)\n",
    " |-- year: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist_id, name, location, lattitude, longitude\n",
    "artist_t = song_df.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"])\n",
    "artist_t.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Extracting the songplay table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark veiw so as to work with dataframes like a table in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplay_id, start_time, user_id, level, session_id, location, user_agent                               song_id, artist_id\n",
    "log_df.createOrReplaceTempView(\"log_df\")\n",
    "song_t.createOrReplaceTempView(\"song_t\")\n",
    "artist_t.createOrReplaceTempView(\"artist_t\")\n",
    "song_df.createOrReplaceTempView(\"song_df\")\n",
    "\n",
    "# user_t\n",
    "# song_t\n",
    "# time_t\n",
    "# artist_t\n",
    "song_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "                SELECT l.ts, l.userId, l.level, l.userAgent, l.sessionId, l.location, s.song_id, s.artist_id\n",
    "                FROM log_df l\n",
    "                LEFT JOIN song_df  s ON l.length = s.duration\n",
    "                LIMIT 5\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "songplays = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT \n",
    "                            l.ts as start_time, \n",
    "                            l.userId as user_id, \n",
    "                            l.level, \n",
    "                            s.song_id, \n",
    "                            s.artist_id, \n",
    "                            l.sessionId as session_id,\n",
    "                            l.location,\n",
    "                            l.userAgent as user_agent                           \n",
    "                FROM \n",
    "                            log_df l\n",
    "                LEFT JOIN\n",
    "                            song_df s ON l.artist = s.artist_name\n",
    "                WHERE  lower(page) = \"nextsong\"\n",
    "\"\"\")\n",
    "songplays.count()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"songplay = {songplays_table.count()}\\nusers = {users_table.count()} \\nsongs = {song_table.count()} \\nartists = {artist_table.count()}\\ntime = {time_table.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Code compliation`\n",
    "The code compliation was a pain in the neck, I will advice to putting the code together as the code shows postive response as you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "import findspark\n",
    "from datetime import datetime \n",
    "\n",
    "# Get spark location on PC\n",
    "SPARK_HOME = os.getenv(\"SPARK_HOME\")\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import MapType, StructType, StructField , DoubleType, IntegerType ,  StringType, TimestampType\n",
    "\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(r\"C:\\DiT\\CS\\Data Engineer\\Module 3 - Data lake\\dl.cfg\")\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a apache spark session.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.2\") \\\n",
    "                .appName(\"Using Spark on S3\") \\\n",
    "                .getOrCreate()\n",
    "    print(\"SparkSession Created!\")\n",
    "    return spark\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), StringType()))\n",
    "def convert_ts(ts):\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "            The 'ts' parameter is a millisecond value from the log_data\n",
    "\n",
    "    Function:\n",
    "            Convert the timestamp in millisecond to the required format i.e\n",
    "            hour, day, week of year, month, year and weekday.\n",
    "    \"\"\"\n",
    "    \n",
    "    t = datetime.fromtimestamp(ts/1000)\n",
    "    try:\n",
    "        hour = t.hour\n",
    "        day =  t.day\n",
    "        week_of_year = t.isocalendar()[1]\n",
    "        month = t.month\n",
    "        year = t.year\n",
    "        weekday =  t.weekday()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return {\"start_time\"   : ts, \n",
    "                'hour'              : hour, \n",
    "                'day'                : day, \n",
    "                'week'              : week_of_year, \n",
    "                'month'            : month, \n",
    "                'year'               : year, \n",
    "                'weekday'         : weekday}\n",
    "\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    This help process the song data into the different dimension table\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark: session\n",
    "        This is the spark session that has been created\n",
    "    input_data: path\n",
    "        This is the path to the song_data s3 bucket.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nRunning process_song_data\")\n",
    "    # making a struct for the columns\n",
    "    songSchema =  StructType([\n",
    "        StructField(\"artist_id\", StringType()),\n",
    "        StructField(\"artist_latitude\", DoubleType()),\n",
    "        StructField(\"artist_location\", StringType()),\n",
    "        StructField(\"artist_longitude\", DoubleType()),\n",
    "        StructField(\"artist_name\", StringType()),\n",
    "        StructField(\"duration\", DoubleType()),\n",
    "        StructField(\"num_songs\", IntegerType()),\n",
    "        StructField(\"song_id\", StringType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"year\", IntegerType()),\n",
    "            ])\n",
    "\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    song_df = spark.read.json(song_data, schema=songSchema)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = song_df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).dropDuplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy('year', 'artist_id') \\\n",
    "                     .parquet(os.path.join(output_data, 'songs/songs.parquet'), 'overwrite')\n",
    "    print(\"songs_table created and save out as parquet\")\n",
    "    \n",
    "    # extract columns to create artists table\n",
    "    artists_table = song_df.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]).dropDuplicates()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(os.path.join(output_data, 'artists/artists.parquet'), 'overwrite')\n",
    "    print(\"artists_table created and save out as parquet\")\n",
    "\n",
    "    return song_df, songs_table, artists_table\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def process_log_data(spark, song_df, input_data, output_data):\n",
    "    print(\"\\Running process_log_data\")\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log_data/*.json'\n",
    "\n",
    "    log_df = spark.read.json(log_data)\n",
    "    log_df = log_df.filter(log_df.page == \"NextSong\")\n",
    "    \n",
    "    users_table = log_df.select([\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\"]).dropDuplicates()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(os.path.join(output_data, 'users/users.parquet'), 'overwrite')\n",
    "    print(\"users_table created and save out as parquet\")\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    ts_col = log_df.select(\"ts\")\n",
    "    ts_col = ts_col.withColumn(\"parsed_ts\", convert_ts(\"ts\"))\n",
    "\n",
    "    #  Process the field name\n",
    "    fields = ['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']\n",
    "    exprs = [f\"parsed_ts['{field}'] as {field}\" for field in fields ]\n",
    "\n",
    "    # extract columns to create time table\n",
    "    time_table = ts_col.selectExpr(*exprs).dropDuplicates() \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month').parquet(os.path.join(output_data, 'time/time.parquet'), 'overwrite')\n",
    "    print(\"time_table created and save out as parquet\")\n",
    "\n",
    "    # create month column from datetime\n",
    "    get_month = udf(lambda x: datetime.fromtimestamp(x / 1000).month)\n",
    "    log_df = log_df.withColumn(\"month\", get_month(log_df.ts))\n",
    "    \n",
    "    # create year column from datetime\n",
    "    get_year = udf(lambda x: datetime.fromtimestamp(x / 1000).year)\n",
    "    log_df = log_df.withColumn(\"year\", get_year(log_df.ts))\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    song_df.createOrReplaceTempView(\"song_df\")\n",
    "    log_df.createOrReplaceTempView(\"log_df\")\n",
    "\n",
    "    songplays_table = spark.sql(\"\"\"\n",
    "                    SELECT DISTINCT \n",
    "                                l.ts as start_time, \n",
    "                                l.userId as user_id, \n",
    "                                l.level, \n",
    "                                s.song_id, \n",
    "                                s.artist_id, \n",
    "                                l.sessionId as session_id,\n",
    "                                l.location,\n",
    "                                l.userAgent as user_agent,\n",
    "                                l.year,\n",
    "                                l.month                               \n",
    "                    FROM \n",
    "                                log_df l\n",
    "                    LEFT JOIN\n",
    "                                song_df s ON l.artist = s.artist_name\n",
    "                    \"\"\")\n",
    "\n",
    "    songplays_table.write.partitionBy('year', 'month').parquet(os.path.join(output_data, 'songplays/songplays.parquet'), 'overwrite')\n",
    "    print(\"songplays_table created and save out as parquet\")\n",
    "\n",
    "    return log_df, users_table, time_table, songplays_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform the following roles:\n",
    "1.) Get or create a spark session.\n",
    "1.) Read the song and log data from s3.\n",
    "2.) take the data and transform them to tables\n",
    "which will then be written to parquet files.\n",
    "3.) Load the parquet files on s3.\n",
    "\"\"\"\n",
    "\n",
    "spark = create_spark_session()\n",
    "input_data = r\"data\\\\\"\n",
    "# input_data = r\"s3://udacity-dend/\"\n",
    "output_data = \"out\"\n",
    "\n",
    "song_df, song_table, artist_table = process_song_data(spark, input_data, output_data)\n",
    "log_df, users_table, time_table, songplays_table = process_log_data(spark, song_df, input_data, output_data)    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bit2952d1fbf4ef4226a2083fe489539590",
   "display_name": "Python 3.8.0 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}