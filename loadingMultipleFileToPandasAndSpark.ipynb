{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and get file locations\n",
    "* Import necessary libraries\n",
    "* get the mother folder of the subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "filepath = '/home/dit/DiT/CS/Data-career/Data_Engineer/data/log_data'\n",
    "\n",
    "# Get all files with the same format - method 1\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    for f in files :\n",
    "        if f.endswith('.json'):\n",
    "            path = os.path.join(root, f)\n",
    "            all_files.append(path)\n",
    "\n",
    "\n",
    "# Get all files with the same format - method 2\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    files = glob.glob(os.path.join(root,'*.json'))\n",
    "    for f in files :\n",
    "        all_files.append(os.path.abspath(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading to `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8056, 18)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "dfs = []\n",
    "for what in all_files:\n",
    "    a = pd.read_json(what, lines=True)\n",
    "    # print(a.shape)\n",
    "    dfs.append(a)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading to `spark`\n",
    "* before running the code below, create `sparkSession` as `spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dfs = []\n",
    "for what in all_files:\n",
    "    a = spark.read.json(what, schema=songSchema)\n",
    "    # print(a.count())\n",
    "    dfs.append(a)\n",
    "\n",
    "song_df =  reduce(lambda first, second: first.union(second), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading multiple file in pandas from `AWS S3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3, s3fs, boto3\n",
    "import pandas as pd\n",
    "bucket = ''\n",
    "\n",
    "#prefix is the subfolder that hold the files\n",
    "all_objects = s3.list_objects(Bucket = bucket, Prefix='cleansed/perfectplay/system-database/cdv-account-privileges-devpp/snapshot_timestamp=2020-11-26T00:00/')\n",
    "\n",
    "dfs = []\n",
    "for obj in all_objects['Contents']:\n",
    "    \n",
    "    path = 's3://cdv-datalake/' + (obj['Key'])\n",
    "    dfs.append(pd.read_csv(path))\n",
    "    \n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit9fbe9b757a6b42fba3541d6d861fafbd",
   "display_name": "Python 3.8.2 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}